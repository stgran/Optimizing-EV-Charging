import pandas as pd
import sqlite3
import logging

'''
METHODS:

DATASET: The dataset contains data regarding how many megawatts (MW) of
electricity were generated by production type in the DK2 bidding zone
(representing the eastern portion of Denmark). This data is broken down
by hour for the entire year of 2020.

This data was downloaded as a .csv from https://transparency.entsoe.eu/generation/r2/actualGenerationPerProductionType/show?name=&defaultValue=true&viewType=TABLE&areaType=BZN&atch=false&datepicker-day-offset-select-dv-date-from_input=D&dateTime.dateTime=21.12.2021+00:00|CET|DAYTIMERANGE&dateTime.endDateTime=21.12.2021+00:00|CET|DAYTIMERANGE&area.values=CTY|10Y1001A1001A65H!BZN|10YDK-2--------M&productionType.values=B01&productionType.values=B02&productionType.values=B03&productionType.values=B04&productionType.values=B05&productionType.values=B06&productionType.values=B07&productionType.values=B08&productionType.values=B09&productionType.values=B10&productionType.values=B11&productionType.values=B12&productionType.values=B13&productionType.values=B14&productionType.values=B20&productionType.values=B15&productionType.values=B16&productionType.values=B17&productionType.values=B18&productionType.values=B19&dateTime.timezone=CET_CEST&dateTime.timezone_input=CET+(UTC+1)+/+CEST+(UTC+2)

'''
def load_dataset(csv_path):
    # Load the data using Pandas
    dataset = pd.read_csv(csv_path)
    return dataset

def clean_dataset(dataset, column_names=None):
    '''
    During cleaning, I will remove columns that are full of n/e and check for n/a presence.
    
    DROP N/E COLUMNS
    I will remove columns that are full of 'n/e' because this type of production does
    not occur in the Eastern Denmark bidding zone.
    These columns are easily identifiable because they have only one unique value: 'n/e'

    CHECK FOR N/A PRESENCE
    'n/a' means that that type of electricity production should be occurring but for some reason,
    the data is missing. For now, I will flag columns that have a higher 'n/a' density than 10%.

    ADJUST MTU FORMAT


    FUTURE IMPROVEMENTS
    With more time and in a situation where we are using real-time data, not just historical,
    I would set up a system to check if currently 'n/e' production types start getting
    produced or vice versa.
    Additionally, 
    '''
    
    # DROP N/E COLUMNS and CHECK FOR N/A
    drop_columns = ['Area'] # List of columns to drop
    for column in dataset: # Iterate over the columns
        unique_vals = dataset[column].unique()
        unique_vals = [unique_val for unique_val in unique_vals if pd.isna(unique_val) == False]

        # Identifying 'n/e' columns
        if len(unique_vals) == 1 and unique_vals[0] == 'n/e': # Check if the column only contains 'n/e'
            drop_columns.append(column) # Track these columns for dropping

        # Flagging columns with a high density of 'n/a'
        if 'n/a' in unique_vals:
            value_counts = dataset[column].value_counts()
            if value_counts.loc['n/a'] > (dataset[column].size * .1):
                na_density = value_counts.loc['n/a']/dataset[column].size
                logging.info(f'n/a density: {na_density}')
    dataset.drop(drop_columns, axis=1, inplace=True) # Drop those target columns

    # Adjust datetime format to be read as datetime by SQLite
    dataset['MTU'] = dataset['MTU'].apply(lambda datetime: adjust_datetime(datetime))

    if column_names:
        dataset.rename(columns=column_names, inplace=True)
    return dataset

def adjust_datetime(datetime_str):
    '''
    Function to adjust the original datetime format to be readable as a datetime
    by SQLite.
    Switches from hour-hour+1 format to hour timestamp.
    Loses timezone. In the future, this strategy could be confusing when being
    used on multiple timezones and would need to be adjusted.
    '''
    YYYY = datetime_str[6:10]
    MM = datetime_str[3:5]
    DD = datetime_str[:2]
    HH = datetime_str[11:13]
    min_sec = '00:00.000'
    adjusted_datetime = f'{YYYY}-{MM}-{DD} {HH}:{min_sec}'
    return adjusted_datetime

def init_db(db_path, table_name):
    # Query to create a table
    create_table_query = f'''
    CREATE TABLE IF NOT EXISTS {table_name} (
        Area TEXT NOT NULL,
        MTU TEXT NOT NULL PRIMARY KEY,
        Biomass REAL,
        FossilGas REAL,
        FossilHardCoal REAL,
        FossilOil REAL,
        Solar REAL,
        Waste REAL,
        WindOffshore REAL,
        WindOnshore REAL
        );
    '''

    # Initiate SQLite database connection object
    db_conn = sqlite3.connect(db_path)
    # If this db does not yet exist, initiating the connection object
    # actually creates it as a new database.

    # Establish cursor object so we can execute SQL code on the db.
    c = db_conn.cursor()

    # Execute the query
    c.execute(create_table_query)

def insert_data(db_path, dataset, table_name, column_name):
    # Query to add a column
    add_column_query = f'''
    ALTER TABLE {table_name} ADD COLUMN
        {column_name} REAL GENERATED ALWAYS AS (Biomass + FossilGas + FossilHardCoal + FossilOil + Solar + Waste + WindOffshore + WindOnshore);
    '''
    
    # Initiate SQLite database connection object
    db_conn = sqlite3.connect(db_path)

    # Write the dataset into the chosen SQLite table
    dataset.to_sql(table_name, db_conn, if_exists='replace', index=False)

    # Establish cursor object so we can execute SQL code on the db.
    c = db_conn.cursor()

    # Execute the query, adding a generated column that calculates total production
    c.execute(add_column_query)